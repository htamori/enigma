{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/htamori/enigma/blob/master/Untitled0.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "93iRVDOkzHty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "bf23100a-0461-4ed5-eb0e-4811fbf95ce4"
      },
      "cell_type": "code",
      "source": [
        "# google-drive-ocamlfuseのインストール\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Colab用のAuth token作成\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Drive FUSE library用のcredential生成\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# drive/ を作り、そこにGoogle Driveをマウントする\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wgF98IX3z8qK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1261
        },
        "outputId": "c5a3e36f-db1c-4dd1-aeca-04797c7fbbc2"
      },
      "cell_type": "code",
      "source": [
        "# https://github.com/kmaehashi/chainer-colab\n",
        "!pip install glob3\n",
        "!pip install sklearn\n",
        "!apt -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!pip install cupy-cuda80 chainer"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glob3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/21/e042ea7bcb917bac5cf2365d532d44efc23650be81456546a6e89e25371e/glob3-0.0.1.tar.gz\n",
            "Building wheels for collected packages: glob3\n",
            "  Running setup.py bdist_wheel for glob3 ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/5c/db/36/aae4683ac0ba95eb154510b48d6bda87fbaac71b6a9b62123d\n",
            "Successfully built glob3\n",
            "Installing collected packages: glob3\n",
            "Successfully installed glob3-0.0.1\n",
            "Collecting sklearn\n",
            "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.19.2)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Running setup.py bdist_wheel for sklearn ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/76/03/bb/589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.\n",
            "Need to get 28.9 MB of archives.\n",
            "After this operation, 71.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libcusparse8.0 amd64 8.0.61-1 [22.6 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libnvrtc8.0 amd64 8.0.61-1 [6,225 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu artful/multiverse amd64 libnvtoolsext1 amd64 8.0.61-1 [32.2 kB]\n",
            "Fetched 28.9 MB in 0s (60.4 MB/s)\n",
            "\n",
            "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package libcusparse8.0:amd64.\n",
            "(Reading database ... 19845 files and directories currently installed.)\n",
            "Preparing to unpack .../libcusparse8.0_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  6%]\u001b[49m\u001b[39m [###.......................................................] \u001b8Unpacking libcusparse8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 12%]\u001b[49m\u001b[39m [#######...................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 18%]\u001b[49m\u001b[39m [##########................................................] \u001b8Selecting previously unselected package libnvrtc8.0:amd64.\n",
            "Preparing to unpack .../libnvrtc8.0_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 25%]\u001b[49m\u001b[39m [##############............................................] \u001b8Unpacking libnvrtc8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 31%]\u001b[49m\u001b[39m [##################........................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 37%]\u001b[49m\u001b[39m [#####################.....................................] \u001b8Selecting previously unselected package libnvtoolsext1:amd64.\n",
            "Preparing to unpack .../libnvtoolsext1_8.0.61-1_amd64.deb ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 43%]\u001b[49m\u001b[39m [#########################.................................] \u001b8Unpacking libnvtoolsext1:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 56%]\u001b[49m\u001b[39m [################################..........................] \u001b8Setting up libnvtoolsext1:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 62%]\u001b[49m\u001b[39m [####################################......................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 68%]\u001b[49m\u001b[39m [#######################################...................] \u001b8Setting up libcusparse8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 75%]\u001b[49m\u001b[39m [###########################################...............] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 81%]\u001b[49m\u001b[39m [###############################################...........] \u001b8Setting up libnvrtc8.0:amd64 (8.0.61-1) ...\n",
            "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 87%]\u001b[49m\u001b[39m [##################################################........] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 93%]\u001b[49m\u001b[39m [######################################################....] \u001b8Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "\n",
            "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[JCollecting cupy-cuda80\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/30/82/9eb531d941b22021ee47e7946a551beca407f532f5834ceb5efad6c1c20d/cupy_cuda80-4.3.0-cp36-cp36m-manylinux1_x86_64.whl (200.4MB)\n",
            "\u001b[K    100% |████████████████████████████████| 200.4MB 146kB/s \n",
            "\u001b[?25hCollecting chainer\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/c6/61ff9041ea7427fc1e39768f740ab8b880f8ef20960a5f791e978e8d81c0/chainer-4.3.1.tar.gz (400kB)\n",
            "\u001b[K    100% |████████████████████████████████| 409kB 12.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80) (1.11.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80) (1.14.5)\n",
            "Collecting fastrlock>=0.3 (from cupy-cuda80)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/24/767ce4fe23af5a4b3dd229c0e3153a26c0a58331f8f89af324c761663c9c/fastrlock-0.3-cp36-cp36m-manylinux1_x86_64.whl (77kB)\n",
            "\u001b[K    100% |████████████████████████████████| 81kB 17.2MB/s \n",
            "\u001b[?25hCollecting filelock (from chainer)\n",
            "  Downloading https://files.pythonhosted.org/packages/2d/ba/db7e0717368958827fa97af0b8acafd983ac3a6ecd679f60f3ccd6e5b16e/filelock-3.0.4.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from chainer) (3.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.0.0->chainer) (39.1.0)\n",
            "Building wheels for collected packages: chainer, filelock\n",
            "  Running setup.py bdist_wheel for chainer ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/8a/ef/b0/e67e0555c4d520566d6565d9634ecb7fbb1594758236bb7b40\n",
            "  Running setup.py bdist_wheel for filelock ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/35/ba/67/4cc48738870c3b54f9e3b5d78bf9de130befb70c1d359faf8b\n",
            "Successfully built chainer filelock\n",
            "Installing collected packages: fastrlock, cupy-cuda80, filelock, chainer\n",
            "Successfully installed chainer-4.3.1 cupy-cuda80-4.3.0 fastrlock-0.3 filelock-3.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fSLDlM8RAFHv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "75f03109-4043-40b5-fb82-c1f428995d67"
      },
      "cell_type": "code",
      "source": [
        "!pip install numpy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.14.5)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MdARiYa7vdVG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "4450d775-a4b9-49a4-cd4e-4618b18f6c03"
      },
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import random\n",
        "import copy\n",
        "import json\n",
        "\n",
        "total = ''\n",
        "chars = 'abcdefghijklmnopqrstuvwxyz., '\n",
        "char_index_0 = dict()\n",
        "char_index_1 = dict()\n",
        "\n",
        "random.seed(7104)\n",
        "tmp_random_0 = random.sample(range(0, 29), 29)\n",
        "random.seed(7105)\n",
        "tmp_random_1 = random.sample(range(0, 29), 29)\n",
        "\n",
        "\n",
        "for i in range(len(chars)):\n",
        "  char_index_0[chars[i]] = tmp_random_0[i]\n",
        "  char_index_1[chars[i]] = tmp_random_1[i]\n",
        "\n",
        "print(char_index_0)\n",
        "print(char_index_1)\n",
        "  \n",
        "for name in glob.glob('drive/Colab Notebooks/corpus/bbc/*/*'):\n",
        "  # print(name)\n",
        "  try:\n",
        "    text = open(name).read()\n",
        "  except Exception as ex:\n",
        "    continue\n",
        "  buff = []\n",
        "  for char in list(text.lower()):\n",
        "    if char in char_index_0:\n",
        "      buff.append(char)\n",
        "  total += ''.join(buff)\n",
        "\n",
        "# print(total)\n",
        "\n",
        "# random slice\n",
        "pairs = []\n",
        "for index in random.sample( list(range(0, len(total) - 150)),100000):\n",
        "  _char_index_0 = copy.copy(char_index_0)\n",
        "  _char_index_1 = copy.copy(char_index_1)\n",
        "  real = total[index:index+150]\n",
        "\n",
        "  enigma = []\n",
        "  for diff, char in enumerate(real):\n",
        "    # roater No.1 update _char_index\n",
        "    _char_index_0 = { char:(ind+1)%len(_char_index_0) for char, ind in _char_index_0.items() }\n",
        "    # get index\n",
        "    ind = _char_index_0[char]\n",
        "    next_char = chars[ind]\n",
        "\n",
        "    # roater No.2\n",
        "    _char_index_1 = { char:(ind+1)%len(_char_index_1) for char, ind in _char_index_1.items() }\n",
        "    # get index\n",
        "    ind = _char_index_1[next_char]\n",
        "    next_char = chars[ind]\n",
        "\n",
        "    enigma.append(next_char)\n",
        "  cript = ''.join(enigma)\n",
        "\n",
        "  crop = random.choice(list(range(len(char_index_0))))\n",
        "  real, cript = real[crop:crop+100], cript[crop:crop+100]\n",
        "  pairs.append( (real, cript) )\n",
        "\n",
        "open('drive/Colab Notebooks/corpus/pairs.json', 'w').write( json.dumps(pairs, indent=2) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 13, 'b': 23, 'c': 4, 'd': 10, 'e': 5, 'f': 15, 'g': 12, 'h': 7, 'i': 22, 'j': 1, 'k': 25, 'l': 27, 'm': 16, 'n': 11, 'o': 28, 'p': 20, 'q': 21, 'r': 19, 's': 6, 't': 14, 'u': 2, 'v': 8, 'w': 0, 'x': 3, 'y': 18, 'z': 24, '.': 9, ',': 17, ' ': 26}\n",
            "{'a': 16, 'b': 28, 'c': 19, 'd': 22, 'e': 24, 'f': 14, 'g': 21, 'h': 4, 'i': 8, 'j': 23, 'k': 5, 'l': 26, 'm': 2, 'n': 9, 'o': 17, 'p': 0, 'q': 1, 'r': 10, 's': 3, 't': 20, 'u': 11, 'v': 6, 'w': 13, 'x': 25, 'y': 12, 'z': 7, '.': 15, ',': 27, ' ': 18}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22400002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "l4BV73Brrcxv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "\n",
        "from chainer import Chain\n",
        "from chainer import cuda\n",
        "from chainer import optimizers\n",
        "from chainer import serializers\n",
        "from chainer import Variable\n",
        "\n",
        "import configparser\n",
        "import cupy\n",
        "\n",
        "class decrypt_enigma(chainer.Chain):\n",
        "  def __init__(self, encode_vocab_size, decode_vocab_size, dim_embed, n_bilstm_layer, n_lstm_layer, n_hidden_layer, n_lstm_dropout):\n",
        "    super(decrypt_enigma, self).__init__(\n",
        "      enc_embed = L.EmbedID(encode_vocab_size, dim_embed),\n",
        "      enc_lstm = L.NStepBiLSTM(n_bilstm_layer, dim_embed, n_hidden_layer, n_lstm_dropout),\n",
        "\n",
        "      dec_embed = L.EmbedID(decode_vocab_size, dim_embed),\n",
        "      dec_lstm = L.NStepLSTM(n_lstm_layer, dim_embed, n_hidden_layer, n_lstm_dropout),\n",
        "      dec_linear = L.Linear(n_hidden_layer, decode_vocab_size),\n",
        "    )\n",
        "    \n",
        "  def __call__(self, encrypted, original_in, original_out):\n",
        "    embedding_encrypted = [self.enc_embed(sent_encrypted) for sent_encrypted in encrypted]\n",
        "    embedding_original_in = [self.dec_embed(sent_original) for sent_original in original_in]\n",
        "\n",
        "    hidden_states, cell_states, _ = self.enc_lstm(None, None, embedding_encrypted)\n",
        "    _, _, embedded_outputs = self.dec_lstm(hidden_states, cell_states, embedding_original_in)\n",
        "    \n",
        "    loss = 0.0\n",
        "    for embedded_output, original in zip(embedded_outputs, original_out):\n",
        "      \n",
        "    \n",
        "    print(len(embedding_encrypted))\n",
        "    print(embedding_encrypted[0].shape)\n",
        "    return None\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3uUxRizv6Jp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "outputId": "2a132d5d-87d8-420a-b5b5-d464777d0bee"
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "with open('drive/Colab Notebooks/corpus/pairs.json', 'r') as f:\n",
        "  json_cont = json.loads(f.read())\n",
        "\n",
        "original = []\n",
        "encrypted = []\n",
        "eos = '<eos>'\n",
        "for pair in json_cont:\n",
        "  tmp_original_in = list(pair[0])\n",
        "  tmp_original_out = list(pair[0])\n",
        "  tmp_encrypted = list(pair[1])\n",
        "\n",
        "  tmp_original_in.insert(0, eos)\n",
        "  tmp_original_out.append(eos)\n",
        "  tmp_encrypted.append(eos)\n",
        "  tmp_original=[tmp_original_in, tmp_original_out]      \n",
        "  original.append(tmp_original)\n",
        "\n",
        "  encrypted.append(tmp_encrypted)\n",
        "\n",
        "print('total size of original data: {}, encrypted: {}'.format(str(len(original)), str(len(encrypted))))\n",
        "\n",
        "char_to_id = {}\n",
        "id_to_char = {}\n",
        "chars = 'abcdefghijklmnopqrstuvwxyz., '\n",
        "for i in range(len(chars)):\n",
        "  char_to_id[chars[i]] = i\n",
        "  id_to_char[i] = chars[i]\n",
        "  \n",
        "char_to_id[eos] = len(chars)\n",
        "id_to_char[len(chars)] = eos\n",
        "\n",
        "ided_original = []\n",
        "ided_encrypted = []\n",
        "for orig_sent in original:\n",
        "  temp_in = []\n",
        "  temp_out = []\n",
        "  for orig_sent in orig_sents:\n",
        "    for orig_char in orig_sent[0]:\n",
        "      temp_in.append(char_to_id[orig_char])\n",
        "    for orig_char in orig_sent[1]:\n",
        "      temp_out.append(char_to_id[orig_char])\n",
        "    temp = [temp_in, temp_out]\n",
        "  ided_original.append(temp)\n",
        "  \n",
        "for enc_sent in encrypted:\n",
        "  temp = []\n",
        "  for enc_char in enc_sent:\n",
        "    temp.append(char_to_id[enc_char])\n",
        "  ided_encrypted.append(temp)\n",
        "\n",
        "  \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_original, t_original, x_encrypted, t_encrypted = train_test_split(ided_original, \n",
        "                                                                    ided_encrypted,\n",
        "                                                                    test_size=0.05)\n",
        "\n",
        "print('size of training data: {}, test_data: {}'.format(str(len(x_original)), str(len(t_original))))\n",
        "print(x_original[0])\n",
        "print(x_encrypted[0])\n",
        "N = len(x_original)\n",
        "N_test = len(t_original)\n",
        "\n",
        "model = decrypt_enigma(30, 30, 128, 1, 1, 512, 0.1)\n",
        "optimizer = optimizers.Adam()\n",
        "optimizer.setup(model)\n",
        "optimizer.add_hook(chainer.optimizer.GradientClipping(5))\n",
        "\n",
        "batch_train = 128\n",
        "batch_test = 1\n",
        "n_epoch = 10\n",
        "\n",
        "cuda.check_cuda_available()\n",
        "cuda.get_device(0).use()\n",
        "xp = cupy\n",
        "if xp == cupy:\n",
        "  model.to_gpu()\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "  perm = np.random.permutation(N)\n",
        "  for i in range(0, N, batch_train):\n",
        "    model.cleargrads()\n",
        "    index = perm[i: (i+batch_train) if (i+batch_train) < N else N]\n",
        "    batch_encrypted = [Variable(xp.array(x_encrypted[j], dtype=xp.int32)) for j in index]\n",
        "    batch_original_in = [Variable(xp.array(x_original[j][0], dtype=xp.int32)) for j in index]\n",
        "    batch_original_out = [Variable(xp.array(x_original[j][1], dtype=xp.int32)) for j in index]\n",
        "\n",
        "    loss = model(batch_encrypted, batch_original_in, batch_original_out)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total size of original data: 100000, encrypted: 100000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-86d0da275791>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0morig_char\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morig_sent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morig_char\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m   \u001b[0mided_original\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "M0IecsTTwL0u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "outputId": "94912518-0f27-4bc6-e6a4-0fff10d2a8d4"
      },
      "cell_type": "code",
      "source": [
        "json_cont"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-278e3759c8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson_cont\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'json_cont' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "ES1vN_tUwMF1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "FVQyfWV2zCo7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RrjAxeZR_5vM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}